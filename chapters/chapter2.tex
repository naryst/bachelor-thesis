\chapter{Literature Review}
~\label{chap:lr}
\chaptermark{Literature Review}


% \Blindtext[2]

% \section{Another Section}
% \Blindtext[1]
% \newpage

\section{Introduction}~\label{sec:lr_intro}
The main focus of this chapter is a comprehensive literature review of the \textbf{Commit Message Generation} (CMG) task. The chapter is meticulously structured as follows: Section 2 furnishes an in-depth exploration of the foundation of Deep Learning in the realm of \textbf{Natural Language Processing} (NLP). Section 3 delineates the list of approaches for the CMG task. Section 4 provides a comprehensive overview of the various datasets created and employed for CMG research. Section 5 offers a thorough examination of the prevailing evaluation metrics used in the assessment of CMG models. Section 6 identifies and elaborates on the significant gaps and deficiencies present within the current research. Section 7 brings this chapter to a conclusion, summarizing the key insights and findings presented within the chapter.


\section{Background of the Deep Learning in the NLP}~\label{sec:lr_background}
Deep learning techniques in NLP have brought remarkable progress and widespread adoption. Deep learning models, particularly those based on transformer architectures{ }\cite{vaswani2017attention}, have revolutionized the field since 2018. Models like \textbf{BERT} (Bidirectional Encoder Representations from Transformers){ }\cite{devlin2018bert}, \textbf{GPT} (Generative Pre-trained Transformer){ }\cite{radford2018improving}, and their derivatives have set new standards for NLP tasks, achieving state-of-the-art results for most of the NLP tasks. Transfer learning is a dominant paradigm, where pre-trained models are fine-tuned for specific tasks, reducing the need for large labeled datasets. These models have made NLP more accessible, enabling researchers and practitioners to build language understanding systems with relatively modest computational resources. Despite these advancements, several challenges remain: mitigating biases,  improving the model's understanding of context and semantics, and hallucinations in the model's answers. Nevertheless, deep learning in NLP has revolutionized this research area, offering unprecedented capabilities for processing and generating natural language.

\section{Approaches to Commit Message Generation}~\label{sec:CMG_approaches}
Almost all modern commit message generation solutions use \textbf{Language modeling} to get the result.\\
In this approach, CMG is represented as a code2text \textbf{Neural Machine Translation} (NMT) task. In this setting, the input and output of the model are considered as a sequence of tokens. Input tokens are represented as $X = (x_1, x_2, \dots x_n)$, which is information about the certain commit. Output tokens are the generated commit message $Y = (y_1, y_2, \dots, y_m)$. Language Modeling model is training to learn a distribution of conditional probabilities of the Commit Message tokens, given the information about the commit $P(y_1,\dots y_m | x_1, \dots x_n)$. 
Modern CMG solutions mostly use encoder-decoder neural network architecture{ }\cite{eliseeva2023commit,liu2020atom,liu2022commitbart,jung2021commitbert}. In this approach, the encoder part transforms input $X$ into a hidden representation $h \in \mathbb{R}^{d}$, and the decoder then, using this $h$ generates an output $Y$. At this point, the structure of the model may vary. The approach might based on the pure transformer model{ }\cite{jung2021commitbert,liu2022commitbart} or \textbf{Graph Neural Network} (GNN) combined with the transformer model{ }\cite{dong2022fira}. Some approaches combine the transformer model with the retrieval mechanism{ }\cite{shi2022race}, and this helps to increase the performance of the method.

\subsection{Transformer based models}\label{sec:transformers_for_cmg}
In this approach, code differences from the commit are treated as a sequence of tokens, and the models excel at capturing the nuances within token sequences. This information is crucial for understanding the specific changes made in the code and their impact on the overall software project. The self-attention mechanism in transformers allows the model to weigh the importance of each token in the context of the entire code snippet, enabling it to identify added or deleted lines, modified functions, and other code alterations. Additionally, transformers can efficiently handle the structure of code changes. This model is capable of ensuring that the generated commit messages are not only accurate but also contextually relevant. 

\subsection{Graph Neural Network approach}~\label{sec:GNN_for_CMG}
The GNN approach in CMG represents an innovative and effective way to tackle the task. Unlike the traditional sequence-based methods, GNNs work with the \textbf{Abstract Syntax Tree} (AST) of the code changes. This representation of the relationships helps the model to capture the code's structural and semantic relationships, often defined as a graph.
GNNs excel in learning from these graph-structured representations, allowing them to understand how code changes affect each other in a complex codebase. The GNN model can propagate information through the graph, aggregating context from neighboring code snippets, and use this rich context to generate more contextually aware commit messages.
In summary, the GNN approach in CMG leverages graph-based representations to enhance the quality and relevance of commit messages, providing developers with a deeper understanding of code changes within the broader context of a software project.

\subsection{Transformer-based architecture with the retrieval}~\label{sec:retrieval_for_cmg}
The transformer-based architecture with retrieval mechanisms is a sophisticated approach that enhances CMG by combining the strengths of the transformer model with retrieval techniques. In this context, the retrieval mechanism allows the model to access and incorporate relevant information from a database of previous commits and corresponding commit messages.
This approach is particularly valuable because it addresses the challenge of generating commit messages that are not only informative but also consistent with past practices and project-specific terminology. The retrieval mechanism can be used to identify and incorporate snippets of text or phrases from historical commit messages that are relevant to the current code changes. This helps in producing commit messages that maintain consistency in terminology and style, which is essential for codebase documentation and understanding.
The transformer-based architecture, with its ability to model code-related information effectively, is well-suited for this task. It can combine the retrieved information with its understanding of the code changes to generate commit messages that are both contextually rich and in line with the development team's conventions.
In summary, the combination of a transformer-based architecture with retrieval mechanisms in CMG results in commit messages that are not only accurate and informative but also consistent with the project's history and established practices, contributing to more effective code documentation and collaboration.

\section{Datasets for CMG}~\label{sec:datasets}
Multiple datasets are available for the generation of commit messages. These datasets can be categorized into two distinct groups: the first category comprises datasets that include pairs of code differences created in the commit and their corresponding commit messages, and the second category encompasses datasets that contain supplementary information related to the commit, such as repository name, commit timestamp, SHA (Secure Hash Algorithm) for unique identification of the specific commit, and other relevant metadata.
\begin{itemize}
    \item The \textbf{CommitBERT\textsubscript{data}} dataset, presented in{ }\cite{jung2021commitbert}, encompasses a collection of 345,000 code modification instances paired with corresponding commit messages. These instances come from 52,000 repositories representing six distinct programming languages (Python, PHP, Go, Java, JavaScript, and Ruby), parsed from Github. It is worth noticing that this dataset exclusively contains information about the altered code segments and does not include any supplementary details regarding the associated commit actions. Unlike other datasets, CommitBERT\textsubscript{data} is specifically tailored to focus only on modified lines within a commit, potentially resulting in the omission of crucial contextual information that may be present in the unaltered sections of the code. Other strict filterings on the data: limitation on the number of files in the code changes; usage of only the first line of the original commit message as a prediction target; collecting the only commit messages, beginning from the verb.

    \item \textbf{ATOM\textsubscript{data}} {-} Liu \textit{et al.} in { }\cite{liu2020atom} meticulously curated a dataset obtained from a selection of 56 Java projects, with project inclusion determined based on the number of stars in the corresponding GitHub repositories. Following the exclusion of commits with ambiguous or irrelevant message content and those devoid of substantive source code modifications, the resulting ATOM\textsubscript{data} corpus encompasses 197,968 commits. This dataset comprises the raw commit records and includes information about the extracted functions influenced by each commit. Beyond the code-related data and the corresponding target commit messages, the dataset incorporates essential metadata such as repository names, commit SHA, and commit timestamps.
    
    \item Multi-programming-language Commit Message Dataset (\textbf{MCMD}) presen-ted at{ }\cite{tao2021evaluation}  tries to mitigate the issues from the previous datasets, including (a) only the Java language; (b) small scale of 20,000â€“100,000 commits; (c) limited information about each commit (hence, no way to trace back to the original commit on GitHub). MCMD collects the data from the five programming languages (Java, C\#, C++, Python, JavaScript), and for each language, the dataset contains commits before 2021 from the most starred projects on GitHub. The total size of the dataset after the collection, filtering, and balancing of the data is  450,000 commits for each language. This dataset also contains additional info about the commit.

    \item \textbf{CommitChronnicle} is the dataset presented in the{ }\cite{eliseeva2023commit}. The authors of this research paper posit that datasets for CMG face not only the issues outlined{ }\cite{tao2021evaluation}, but also significant alterations to the original commit history and stringent data filtering. In the CommitChronnicle dataset, the majority of the filters previously applied in prior datasets are eliminated, resulting in increased data diversity and enhanced dataset generality. Notably, CommitChronnicle encompasses 10.7 million commits after the application of various filtration steps. Additionally, this dataset incorporates supplementary information about commits, facilitating the tracking of commit history for specific users and projects during the training process. Leveraging this supplementary information, it becomes possible to generate commit messages that exhibit greater alignment with the project's historical context. In conclusion, CommitChronnicle stands out as the most comprehensive and diverse dataset available for the CMG task, successfully addressing the issues present in previous datasets while offering a significantly larger volume of data.
\end{itemize}

\section{Evaluation metrics for CMG}~\label{sec:eval_metrics}
Numerous metrics are commonly used to assess the performance of CMG methods. Foremost among these is BLEU{ }\cite{papineni2002bleu}, a metric widely adopted in the evaluation of machine translation models. Given that CMG can be conceptualized as a code-to-text task analogous to a neural machine translation, BLEU, and related metrics find relevance in evaluating CMG methods. B-Norm, a variant of BLEU, has been demonstrated to align most closely with human judgments regarding the quality of commit messages, as observed by Tao \textit{et al.}{ }\cite{tao2021evaluation}.
BLEU, reliant on the precision concerning shared n-grams between generated and reference sequences, has been a staple in prior commit message generation studies. \\
Nevertheless, BLEU presents limitations in assessing the quality of the generated text. This metric lacks sensitivity to the semantic nuances embedded within generated text and is incapable of capturing the semantic similarity between the generated output and reference text. To address these limitations, more advanced metrics rooted in neural networks have been developed. Notably, BERTScore{ }\cite{zhang2019bertscore}, which leverages the BERT model, exhibits heightened sensitivity to the semantic nuances of generated text. Despite these advantages, BERTScore has seen relatively limited adoption in the evaluation of CMG methods.

\section{Research gaps}~\label{sec:research_gap}
Despite the remarkable progress in CMG in recent years, several challenges remain. 
\subsection{Limited context window}
Modern CMG methods are limited in capturing the broader context of code changes. They can handle only small pieces of code changes, typically limited to a single file. It's a significant limitation, as code changes often span multiple files, and the context of these changes is crucial for generating accurate commit messages.

\subsection{Stalling performance on the out-of-filter data}
Even in the CommitChronnicle dataset, which is the most comprehensive and diverse dataset available for the CMG task, the performance of modern methods is stalling on the out-of-filter data. It is a significant limitation, as the out-of-filter data is the most relevant for real-world applications.

\subsection{Lack of the evaluation metrics}
Metrics used for NMT can not evaluate the generated commit messages in terms of semantics. Most of the metrics are based on the BLEU, which is not sensitive to the semantic nuances of the generated text. It is a significant limitation, as the semantic nuances are crucial for the commit messages.

\subsection{Lack of the research on CMG using the LLM}
Current SOTA for most of the NLP tasks uses the encoder-based transformer models. For example {-} CodeLLaMa{ }\cite{roziere2023code} {-} is the current SOTA for the code generation task. However, LLM usage for the CMG task does not have enough exploration. Authors in{ }\cite{eliseeva2023commit} attempt to approach the CMG using the ChatGPT with a well-chosen prompt. However, this approach performs worse than the fine-tined encoder-decoder models. The results are the following since  ChatGPT is not trained for this task. It is a significant limitation, as the LLMs are the current SOTA for most NLP tasks.

\section{Conclusion}~\label{sec:lr_conclusion}
This chapter offers a comprehensive review of prior research related to the CMG task. The sections above describe the backdrop of Deep Learning within the realm of NLP, explore various approaches employed for CMG, examine the available datasets tailored for this task, scrutinize the evaluation metrics applied to assess CMG models, identify areas of deficiency in the current body of research, and provided a list of pertinent references. In the subsequent chapter, we will expound upon our proposed solution for the CMG task.
