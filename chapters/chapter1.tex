\chapter{Introduction}\label{chap:intro}

\section{Problem statement}
In software development, commit is an operation that captures the current state of a project's files. It creates a snapshot of code modifications in the project, allowing developers to record their progress. Commits enable the tracking of specific changes, rolling back to previous states, and collaborating within a team of developers.
A commit message is a brief description attached to each commit explaining the changes made. This message is critical for understanding the purpose of the changes. Most of the time, manual writing of commit messages is a tedious task and may take up to several minutes of developer time. 

The task of automatically generating commit messages is the perspective research topic. The writing of concise commit messages is a crucial task in the software development process. They serve as a valuable tool for developers to track changes in the project and to collaborate effectively. Despite this, a significant proportion of commit messages in open source are under-informative. State-of-the-art approaches to commit message generation (CMG) show promising results. However, most current research proposes methods that involve training encoder-decoder transformer-based models for this task. And one of the limitations of current CMG methods is the inability to handle the full context of code changes for the big commits, due to the limited context window of the model.

\section{Methods}
Analyzing the literature on the topic of automatic commit message generation, I concluded that most modern deep learning methods can be divided into two main groups: (\rom{1}) Those that utilize the structured format of code dependencies and use graph neural networks to obtain an embedded representation of code changes. (\rom{2}) Approaches that consider the generation of commit messages as a neural machine translation seq2seq task, take code changes as unstructured text, and solve the task with a causal language modeling objective. 

Another possible but less explored method for this task is to use a large language model, such as ChatGPT4~\cite{achiam2023gpt} or LLaMa~\cite{roziere2023code}, that is tuned for general-purpose instructions. These models have gained popularity fairly recently. Therefore, research in the field of generating commit messages using this method has not yet been sufficiently studied. The disadvantage of this approach is the excessive number of parameters in the models, leading to slow inference and training times that require more computational resources. Nevertheless, even without specific fine-tuning general purpose large language models show promising results in the commit messages generation. I want to clarify that I will not use large language models in my work. I assume that automating the process of writing commit messages should save the time of developers. Therefore, it should be done quickly and without requiring a huge amount of resources.  Large language models mostly achieve their performance due to the scaling in terms of parameters and therefore do not fit into these conditions.  


\section{Work objective}
The primary focus of my work is on experiments with transformer-based encoder-decoder models training for the task of generating commit messages. It includes four different approaches to solving this task with causal language modelling.
The first experiment is a straightforward training of the base model to obtain the baseline. Then, experiments are provided with the model scaling in terms of parameters. With this, I investigated how it will affect the task in terms of the quality of the generated commit message, the time efficiency, and the required memory. \\
In the next experiment, I attempted to mitigate the common problem with the generation of commit messages, style adoption. In the literature, this issue is typically resolved with the help of retrieval methods. Therefore, I modified the baseline to insert the retrieval mechanism in it. This modification enables the model to adjust generated messages to the style of commit messages in the repository. My retrieval method also leverages the model's ability to generate descriptive messages by getting the human written message from the commit with a similar semantic obtained from the database. \\
In the further experiment, I tried to mitigate the well-known problem of long context processing. Ð¡ommits can impact multiple files, each of which may contain a significant amount of code. Due to this limited context window of the transformer model, it cannot always process the entire commit. One of the possible way to deal with this problem is to consider each file from the commit separately. In my method I implemented additional module between encoder and decoder parts of the baseline transformer model. First, each file goes through the encoder separately. After processing all the files for the commit, a weighted average between them is taken to get the final hidden representation for the decoder, as in the standard encoder-decoder model. This scores for the averaging are trainable parameters and represents the importance of the file in the commit. 

Another part of my work, separated from the deep learning models training includes data collection and preparation and metrics analysis. From the data perspective my work includes the collecting my own dataset of commits from the GitHub and the analysis of data from other works. My process of data selection also includes comparison of the dataset and collection of statistics to came to the final decision about the data that will be used during the training process. \\
And regarding the metrics, In my work I analysed most widely used metrics from the literature and the idea behind them. Additionally I provided some analysis to statistically prove that the metrics I choose correlate with the quality of the generated commit message.