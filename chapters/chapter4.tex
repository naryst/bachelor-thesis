\chapter{Implementation}\label{chap:impl}

\section{Vanilla CodeT5+ training details}
The theoretical aspects of training the transformer-based encoder-decoder model for the commit message generation task were described in the corresponding methodology section \ref{sec:codeT5_train}, and in this part of the work, I will give details of the implementation of the training process. This section includes details of the model input sequence format and the representation of the label vector to compute the loss function. Further, I will describe the hyper-parameters required for the model training and inference and the logic behind them.
\subsection{Input sequences representation}
The first step in training a neural network involves preparing and converting the data to the input format of the model. CommitChronicle dataset stores code changes in the format of JSON. It is required to convert it into plain text and insert "special" tokens in the data preprocessing stage, according to the desired format \ref{sec:structure_of_the_data}. 
The next step in data preparation is to tokenize the text. Machine learning models cannot process data in text format because they can only work with numeric values.
Initially, I needed to convert text into numbers. The standard method is to use a pre-trained tokenizer that's trained with the model. The tokenizer contains a vocabulary with the mapping of words to numbers. The model takes a sequence of numbers as input, each corresponding to a word in the dictionary. \\
In neural network training, data is typically passed in batches rather than as a single sequence. Batched training is utilized to accelerate GPU computations and enhance loss convergence. For this reason, the input batch should have a form of the matrix. The issue with constructing this matrix arises due to the varying lengths of input sequences. This condition makes it impossible to unify the shape of the batch matrix without additional data transformation. The common technique to resolve this issue in the sequence processing task is to add padding tokens to each sample to equalize all sequences in length. These padding tokens are excluded from the model's self-attention calculation, so expanding the sequence length does not result in additional computations. For my experience with CodeT5+, I expanded all the input sequences to the maximum size of the model context - 512 tokens. The same padding should be applied to the label vectors. The difference is that the cross-entropy loss interprets the padding as correctly guessed tokens, thus making the loss function less sensitive. To avoid this loss degradation, I excluded the padding tokens while computing it. \\
All the techniques described in this section are required for the efficient interpretation of the data by the model. Furthermore, this preprocessing enables the batched model training and the efficient loss computation.

\subsection{Training hyper-parameters}
The choice of hyper-parameters is the crucial step in neural network training. A model's performance in a given task is significantly impacted by a set of hyperparameters that affect the convergence of the loss function. In this section, I will describe all the hyperparameters chosen for fine-tuning the CodeT5+ model for the commit message generation task. In addition, I will define each parameter and explain how they affect the fine-tuning process.

\begin{table}[h]
    \centering
    \caption{Fine-tuning hyper-parameters choice}\label{tab:ft_hyperparams}
    \renewcommand{\arraystretch}{1.5} % Adjusts the row height
    \begin{tabular}{| l | c |} % chktex 44
    \hline % chktex 44
    \textbf{Hyper-parameter} & \textbf{value} \\
    \hline % chktex 44
    batch size      & $32$ \\ \hline  
    optimizer       & AdamW \\ \hline  
    learning rate     & $2 \times 10^{-5}$ \\ \hline  
    learning rate scheduler & linear \\ \hline
    weight decay       & $10^{-2}$ \\ \hline  
    warm-up steps & $100$ \\ \hline  
    warm-up strategy & linear \\ \hline
    bf16    & True \\ \hline  
    \end{tabular}
\end{table}
Table~\ref{tab:ft_hyperparams} represents a list of all the hyper-parameters configuration for \\ CodeT5+ fine-tuning. The \textbf{batch size} for training and evaluation was set to 32. According to empirical findings, I get that this batch size is optimal in training time and computation resource usage. As an \textbf{optimizer} I chose an improved version of Adam Optimizer~\cite{kingma2014adam} that decouples weight decay from the optimization steps, applying it directly to the weights - AdamW~\cite{loshchilov2017decoupled}. In recent research, it has been shown that this optimizer leads to better convergence in the task of causal language modeling. For the \textbf{learning rate} I set the maximum value to be $2 \times 10^{-5}$. The learning rate can be interpreted as a "step size" for the gradient descent method. In the general form, it corresponds to $\gamma$ in model parameters update formula $w_{\text{new}} = w_{\text{old}} - \gamma \nabla f(w_{\text{old}})$. During the training, the learning rate is configured using the \textbf{linear scheduler}, which monotonically decreases the learning rate to 0 till the end of the training. One more parameter to configure the learning rate is the \textbf{linear warm-up}, which monotonically increases it to the maximum specified by the current from 0 during the \textbf{warm-up steps}. \text{Weight decay} is the hyperparameter that is responsible for neural network parameters regularization. The optimization objective then takes the form of $L_{\text{new}(w)} = L_{\text{original}}(w) + \lambda w^Tw$, where $\lambda$ is the specified strength of the weight decay penalty. Hyperparameter \textbf{bf16} indicates that instead of full precision with 32-bit float model parameters and optimizer states, I used brain float with 16 bits. Bfloat differs from the usual half-precision float in that it has 8 bits in the exponent part, while fp16 has only 5 bits. With this modification, bfloat16 can handle the same range of numbers as a full-precision float with 32 bits but with less precision than fp16, as it has fewer bits in mantissa. Recent research~\cite{kalamkar2019study} has shown that training neural networks with bf16 achieves the same performance as full precision, but with fewer computational resources required.  \\
All the hyper-parameters described above are used to configure the training  process. The set of parameters from this section, except for batch size, is shared across all models trained in my work. This set of parameters was taken from~\cite{eliseeva2023commit}, as authors of this work achieved SOTA on the commit message generation task using the same dataset.

\subsection{Generation hyper-parameters}
The output of a trained language model represents the probability distribution of the next text token. To construct an output, the model predicts all tokens in an autoregressive manner. This means that when predicting the output $\hat{Y}$, $\hat{y}_n$ depends on the previously generated context $(\hat{y}_0 \dots \hat{y}_{n-1})$. One of the possible ways to construct an output sequence is to choose the most probable token every time. But this greedy strategy may lead to the generated text degradation. There exist methods to make the generated text more human-readable and increase the cumulative probabilities of the choices. In this section, I will describe the generation hyperparameters I used for my model and explain the intuition behind them to justify my choice.

\begin{table}[h]
    \centering
    \caption{Generation hyper-parameters choice}\label{tab:gen_hyperparams}
    \renewcommand{\arraystretch}{1.5} % Adjusts the row height
    \begin{tabular}{| l | c |} % chktex 44
    \hline % chktex 44
    \textbf{Hyper-parameter} & \textbf{value} \\
    \hline % chktex 44
    max new tokens & $128$ \\ \hline
    top k & $100$ \\ \hline
    number of beams & $5$ \\ \hline
    early stopping & True \\ \hline
    no repeat n-gram size & $2$ \\ \hline
    do sample & True \\ \hline
    top p & $0.95$ \\ \hline 
    \end{tabular}
\end{table}

Table~\ref{tab:gen_hyperparams} lists the set of hyperparameters that I use to generate commit messages. \textbf{Max new tokens} parameter configures the maximum length of the generated message. According to the training dataset statistics, the commit messages are $\sim60$ characters on average. To make the results similar to the actual message I limited the maximum number of generated tokens. \textbf{Top k} parameter is responsible for the number of tokens that the model considers in the prediction. It makes the model choose only among the k most probable words. This technique prevents the model from hallucinating by randomly selecting the inappropriate token, making the generation more stable. The parameter \textbf{ number of beams} refers to the beam search decoding strategy. It implies the use of heuristics to make the output more diverse. Each beam starts from the initial position and predicts the next token according to the probabilities of the model. At the end of the generation, beams are reranked according to the cumulative probability of the predicted sequences. \textbf{Early stopping} flag also corresponds to beam search. It controls that the generation process should stop after the first \textbf{num\_beams} beams are done. 
I also prevented the model from generation of the same bi-gram several times. It is a known issue of the large language models that they may hallucinate and start repeating some phrase till the end of the response. 
\textbf{Do sample} parameter just states what tokens are selected according to the probabilities, and not just with a greedy strategy.
\textbf{Top p} parameter corresponds to the nucleus sampling method proposed in~\cite{holtzman2019curious}. According to this method only the smallest set of most probable tokens with probabilities that add up to \textbf{top\_p} or higher are kept for generation. This technique is similar to the \textbf{top\_k} and prevent the model from hallucinating and predicting irrelevant tokens.  \\ 
All of the generation hyperparameters described in this section helps the model to generate more relevant text. Moreover, some of the parameters, like sampling, make the generated commit message more human readable. 

\subsection{Training process}

\section{CodeT5+ with retrieval training results}

\section{CodeT5+ with file attention training results}

\section{Analysis of BERTScore metric}
