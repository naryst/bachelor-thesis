\begin{abstract}
\textbf{Background:} The task of automatically generating commit messages is the perspective research topic. Writing concise commit messages is a crucial task in the software development process. They serve as a valuable tool for developers to track changes in the project and to collaborate effectively. Despite this, a significant proportion of commit messages in open source are under-informative. State-Of-The-Art approaches to commit message generation (CMG) show promising results. However, most current research proposes methods that involve training encoder-decoder transformer-based models for this task. And one of the limitations of current CMG methods is the inability to handle the full context of code changes for the big commits, due to the limited context window of the model.

\textbf{Methods:} The given task can be formulated as code2text language modelling. And the most popular method for this at the moment is fine-tuning transformer-based encoder-only Large Language Models (LLM) for the given task. So in my work I have fine-tuned LLM, pre-trained it on the massive code data and compared this solution with the existing ones. And to deal with the problem of long context, I used methods that allow to extract useful information from the long data sequences and pass it to the model input instead of the raw code context.

\textbf{Results:} Using encoder-only model with much more parameters improved the result of generating commit messages. Methods for extracting embeddings from the code and using them instead of the raw code give an improvement in terms of the meaning of the result. However, the computation time for commit message generation increased due to the size of the model.

\textbf{Contribution and applicability:} The methods proposed in this work increase the performance in the CMG task, but at the same time the time efficiency of the solution became worse. And the problem of finding a balance between running time and prediction quality requires future work.

\end{abstract}