\chapter{Introduction}\label{chap:intro}

\section{Problem statement}
In software development, commit is an operation that captures the current state of a project's files. Commits creates a snapshot of code modifications in the project, allowing developers to record their progress. They enable the tracking of specific changes, rolling back to previous states, and collaborating within a team of developers.
A commit message is a brief description attached to each commit explaining the changes made. This message is critical to understanding the purpose of the changes. Most of the time, manual writing of commit messages is a tedious task and may take up to several minutes of developer time. 

The writing of concise commit messages is a fundamental task in the software development process. They serve as a valuable tool for developers to track changes in the project and to collaborate effectively. Despite this, a significant proportion of open source commit messages are under-informative. State-of-the-art approaches to commit message generation (CMG) show promising results. However, most current research proposes methods that involve training encoder-decoder transformer-based models for this task. 
Limitations of current CMG methods are the inability to handle the full context of code changes for the big commits, due to the limited context window of the model, and problems with predicting on the noisy data. 

\section{Methods}
Analyzing the literature on the topic of automatic commit message generation, I derived that most modern deep learning methods can be divided into two main groups: (\rom{1}) Those that utilize the structured format of code dependencies and use graph neural networks to obtain an embedded representation of code changes. (\rom{2}) Approaches that consider the generation of commit messages as a seq2seq neural machine translation task, take code changes as unstructured text, and train the model with a causal language modeling objective. 

% Another possible but less explored method for this task is to use a large language model, such as ChatGPT4~\cite{achiam2023gpt} or LLaMa~\cite{roziere2023code}, that is tuned for general-purpose instructions. These models have gained popularity fairly recently. Therefore, research in the field of generating commit messages using this method has not yet been sufficiently studied. The disadvantage of this approach is the excessive number of parameters in the models, leading to slow inference and training times that require more computational resources. Nevertheless, even without specific fine-tuning general purpose large language models show promising results in the commit messages generation. I want to clarify that I will not use large language models in my work. I assume that automating the process of writing commit messages should save developers time. Therefore, it should be done quickly and without requiring a large amount of resources.  Large language models mostly achieve their performance due to the scaling in terms of parameters and therefore do not fit into these conditions.  

\section{Work objective}
The primary focus of my work is on experiments with transformer-based encoder-decoder models training for the task of generating commit messages. It includes four different approaches to solving this task with causal language modeling.
The first experiment is a straightforward training of the base model to obtain the baseline. Then, experiments are provided with the model scaling in terms of parameters. With this, I investigated how it will affect the task in terms of the quality of the generated commit message, the time efficiency, and the required memory. 
In the next experiment, I attempted to mitigate the common problem with the generation of commit messages, style adaption. In the literature, this issue is typically resolved with the help of retrieval methods. Therefore, I modified the baseline to insert the retrieval mechanism in it. This modification enables the model to adjust the generated messages to the style of the commit messages in the repository. My retrieval method also leverages the model's ability to generate descriptive messages by getting the human written message from the commit with a similar semantic obtained from the database. 
In the further experiment, I tried to mitigate the well-known problem of long context processing. Ð¡ommits can impact multiple files, each of which may contain a significant amount of code. Due to this limited context window of the transformer model, it cannot always process the entire commit. One of the possible ways to deal with this problem is to consider each commit file separately. In my method, I implemented an additional module between the encoder and decoder parts of the baseline transformer model. First, each file goes through the encoder separately. After processing all the files for the commit, a weighted average between them is taken to get the final hidden representation for the decoder, as in the standard encoder-decoder model. These scores for averaging are trainable parameters and represent the importance of the file in the commit. 

Another part of my work, separate from the deep learning model training, includes data collection and preparation, and metrics analysis. From the data perspective, my work includes collecting my own dataset of commits from GitHub and the analysis of data from other works. My data selection process also includes comparison of the dataset and collection of statistics to came to the final decision about the data that will be used during the training process. 
And regarding the metrics, In my work, I analyzed most widely used metrics from the literature and the idea behind them. To have a complete analysis of the model performance, I used an additional metric for the semantic similarity measurement. Despite the infrequent use of this kind of metrics in current research, it is proved that this metric outperforms the existing statistical metrics in the relations with the human judgement.  