\chapter{Conclusion}\label{chap:conclusion}
The purpose of this chapter is to summarize all the work done in my thesis. This chapter finalizes the results of all the experiments conducted. Furthermore, it underscores the broad impact of my research in the domain of automatic commit message generation. 
In addition, this chapter considers the possible extension of the experiments performed in this work. 

\section{Summary and main findings}
Regarding the models training, my work includes three different experiments: (\rom{1}) Scaling the base model to access the performance improvement, (\rom{2}) Addition of the retrieval mechanism to the base model, (\rom{3}) Addition of the file attention mechanism to the base model. The results of these experiments can be summarized in the following way. 
Scaling the model from 220 million to 770 million parameters improved the quality of the generated messages and demonstrated the best performance across all experiments. This modification of the model increased the average time to process the input; however, the time efficiency is still at a level sufficient for effective utilization. 
Adding the retrieval mechanism to the base model also increased the processing time due to the bottleneck in the step of retrieving from the index and receiving a response from the GitHub API. However, this method helped the model to adapt to the style of the commit messages and improved the performance of the model. 
Analyzing the mechanism of file attention invented in the course of this work, this method has not lived up to initial expectations. Despite the degradation in efficiency and the more computational power spent training, this model achieves the same results as a base CodeT5 trained in another work. Initial assumptions about efficient management of large commits were not confirmed as there was no change in the distribution of the metric when analyzing the group of commits that included multiple files. 

Another research question considered in my work is the specific format of the input and output sequences. More precisely, I tested special tokens that represent the logic of file names, modified lines, and commit messages. According to the original hypothesis, modifying the data in this way is expected to enhance the model's understanding of the semantics involved in code changes. During the fine-tuning, the model should learn how to construct a representative embedding for the special tokens, which will get the model's attention in the right direction. However, the evaluation results showed that this method leads to performance degradation of the model. Taking into account the fact that the training set is approximately three billion tokens, which is enough to train several special token representations, there is no problem with underfitting in this method. Therefore, the conclusion about this method is that the transformer architecture is powerful enough to learn the code modification representation without explicit instructions. In this scenario, special tokens are redundant elements that reduce the available space for code data within the context window.

One more novelty presented in my work is the set of metrics used to assess the performance of the models. In the section describing research gaps, a particular issue was highlighted: the absence of semantic metrics used to evaluate commit message generation models. To address this problem, my research incorporates the BERTScore metric, described in~\ref{subec:bertscore}. The authors of this method in~\cite{zhang2019bertscore} state that this method outperforms the BLEU score and is more correlated with human judgement. In the course of my work, I ensured that the BERTScore correlates with the quality of the commit messages. To do this, I conducted an experiment by comparing the BERTScore of the generated answer and a random irrelevant commit message. The results of this experiment are shown in the Appendix~\ref{chap:bertscore_analysis}. This metric calculates the semantic similarity of tokens instead of the exact match for the BLEU score and, therefore, is more robust for the considered task.

All the points described above show the importance of this work in the sphere of commit messages generation task. In general, this research contributes valuable insights and methodologies to this field. This work highlights the importance of model scalability, semantic evaluation metrics, and the limitations of certain architecture modifications.

\section{Future work}
The work done leaves great room for further development of the presented methods. To address the shortcomings identified in this study, it is recommended to retrain all models without the use of special tokens. An additional possible direction for future research could involve altering the file attention mechanism. Given that the method did not yield satisfactory results in this study, it requires a further in-depth analysis. There are also some possible improvements to the architecture of this model. For example, a combination of the file attention and retrieval models can be implemented. In this setup, the retrieved passages will be considered as separate entries of the commit and involved in the process of weighted averaging of embeddings. This approach could potentially improve the quality of the messages generated without the efficiency losses observed with model scaling.